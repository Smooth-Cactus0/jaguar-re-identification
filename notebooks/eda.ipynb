{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaguar Re-ID: Exploratory Data Analysis\n",
    "\n",
    "**Competition**: [Jaguar Re-ID](https://www.kaggle.com/competitions/jaguar-re-id)  \n",
    "**Task**: Predict pairwise similarity scores for 137,270 test image pairs  \n",
    "**Metric**: Identity-balanced Mean Average Precision (mAP)  \n",
    "\n",
    "This notebook covers:\n",
    "1. Dataset overview and class distribution\n",
    "2. Sample image gallery per identity\n",
    "3. Image property analysis (resolution, aspect ratio)\n",
    "4. Near-duplicate detection via perceptual hashing\n",
    "5. Test pair structure and positive/negative ratio\n",
    "6. Color and texture characteristics per identity\n",
    "7. Evaluation metric walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('../')\n",
    "TRAIN_DIR = ROOT / 'train' / 'train'\n",
    "TEST_DIR  = ROOT / 'test'  / 'test'\n",
    "FIGURES   = ROOT / 'figures'\n",
    "FIGURES.mkdir(exist_ok=True)\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "PALETTE = sns.color_palette('tab20', 31)\n",
    "\n",
    "print('Setup complete.')\n",
    "print(f'Train dir: {TRAIN_DIR} — exists: {TRAIN_DIR.exists()}')\n",
    "print(f'Test dir:  {TEST_DIR}  — exists: {TEST_DIR.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(ROOT / 'train.csv')\n",
    "test_df  = pd.read_csv(ROOT / 'test.csv')\n",
    "\n",
    "print('=== TRAIN ===' )\n",
    "print(f'  Images     : {len(train_df):,}')\n",
    "print(f'  Identities : {train_df.ground_truth.nunique()}')\n",
    "print()\n",
    "print('=== TEST ===' )\n",
    "print(f'  Pairs      : {len(test_df):,}')\n",
    "test_images = pd.unique(test_df[['query_image', 'gallery_image']].values.ravel())\n",
    "print(f'  Unique imgs: {len(test_images)}')\n",
    "print()\n",
    "\n",
    "# Count per identity\n",
    "identity_counts = train_df['ground_truth'].value_counts()\n",
    "print('=== IDENTITY DISTRIBUTION ===')\n",
    "print(identity_counts.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution\n",
    "\n",
    "The dataset has significant class imbalance — from 13 images (Bernard, Ipepo) to 183 images (Marcela). The identity-balanced mAP metric compensates for this by giving each jaguar equal weight, so **rare jaguars matter just as much as common ones**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "colors = [PALETTE[i] for i in range(len(identity_counts))]\n",
    "bars = ax.bar(identity_counts.index, identity_counts.values, color=colors, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Annotate count on each bar\n",
    "for bar, val in zip(bars, identity_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,\n",
    "            str(val), ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "\n",
    "# Reference lines\n",
    "mean_count = identity_counts.mean()\n",
    "ax.axhline(mean_count, color='crimson', linestyle='--', linewidth=1.2, label=f'Mean: {mean_count:.0f}')\n",
    "\n",
    "ax.set_xlabel('Jaguar Identity', fontsize=11)\n",
    "ax.set_ylabel('Number of Training Images', fontsize=11)\n",
    "ax.set_title('Training Set: Images per Jaguar Identity', fontsize=13, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'eda_01_class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Imbalance ratio (max/min): {identity_counts.max() / identity_counts.min():.1f}x')\n",
    "print(f'Jaguars with < 20 images: {(identity_counts < 20).sum()}')\n",
    "print(f'Jaguars with > 100 images: {(identity_counts > 100).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Image Gallery\n",
    "\n",
    "Let's look at representative images from each jaguar to understand visual variability: spot patterns, viewpoints, lighting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, size=(128, 128)):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img.thumbnail(size, Image.LANCZOS)\n",
    "    # Paste onto white background to normalize size\n",
    "    bg = Image.new('RGB', size, (255, 255, 255))\n",
    "    offset = ((size[0] - img.width) // 2, (size[1] - img.height) // 2)\n",
    "    bg.paste(img, offset)\n",
    "    return bg\n",
    "\n",
    "identities = identity_counts.index.tolist()\n",
    "n_ids = len(identities)  # 31\n",
    "n_cols = 6\n",
    "n_rows = (n_ids + n_cols - 1) // n_cols  # ceil\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2.2, n_rows * 2.5))\n",
    "fig.suptitle('One Representative Image per Jaguar Identity', fontsize=13, fontweight='bold', y=1.01)\n",
    "\n",
    "for idx, identity in enumerate(identities):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Pick the first image for this identity\n",
    "    sample_file = train_df[train_df.ground_truth == identity]['filename'].iloc[0]\n",
    "    img = load_img(TRAIN_DIR / sample_file)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    n = identity_counts[identity]\n",
    "    ax.set_title(f'{identity}\\n({n} imgs)', fontsize=8, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide unused axes\n",
    "for idx in range(n_ids, n_rows * n_cols):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'eda_02_identity_gallery.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Image Grid per Identity\n",
    "\n",
    "Now let's look at **multiple images per jaguar** to understand intra-identity variability — the core challenge of re-ID. We show 4 images per jaguar (where available) for the 10 most-represented individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = identity_counts.head(10).index.tolist()\n",
    "n_show = 5  # images per identity\n",
    "\n",
    "fig, axes = plt.subplots(len(top10), n_show, figsize=(n_show * 2.2, len(top10) * 2.3))\n",
    "fig.suptitle('Intra-Identity Variability: 5 Images per Jaguar (Top 10 by Count)', \n",
    "             fontsize=12, fontweight='bold', y=1.005)\n",
    "\n",
    "for row, identity in enumerate(top10):\n",
    "    files = train_df[train_df.ground_truth == identity]['filename'].tolist()\n",
    "    # Sample spread across the full list to capture diverse shots\n",
    "    indices = np.linspace(0, len(files)-1, n_show, dtype=int)\n",
    "    sampled = [files[i] for i in indices]\n",
    "    \n",
    "    for col, fname in enumerate(sampled):\n",
    "        ax = axes[row, col]\n",
    "        img = load_img(TRAIN_DIR / fname)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(identity, fontsize=9, fontweight='bold', rotation=0, \n",
    "                          labelpad=50, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'eda_03_intra_identity_variability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Property Analysis\n",
    "\n",
    "Check resolution distribution and aspect ratios — important for choosing input size and understanding if resizing will cause significant distortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scanning image properties (this may take ~30 seconds)...')\n",
    "\n",
    "records = []\n",
    "for fname in train_df['filename']:\n",
    "    path = TRAIN_DIR / fname\n",
    "    with Image.open(path) as img:\n",
    "        w, h = img.size\n",
    "        mode = img.mode\n",
    "    records.append({'filename': fname, 'width': w, 'height': h, 'mode': mode,\n",
    "                    'aspect': w / h, 'mpx': w * h / 1e6})\n",
    "\n",
    "props = pd.DataFrame(records)\n",
    "props = props.merge(train_df, on='filename')\n",
    "\n",
    "print(f\"\\nImage modes: {props['mode'].value_counts().to_dict()}\")\n",
    "print(f\"\\nResolution stats:\")\n",
    "print(props[['width','height','aspect','mpx']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Width distribution\n",
    "axes[0].hist(props['width'], bins=40, color='steelblue', edgecolor='white', linewidth=0.5)\n",
    "axes[0].set_xlabel('Width (px)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Image Width Distribution')\n",
    "\n",
    "# Height distribution\n",
    "axes[1].hist(props['height'], bins=40, color='coral', edgecolor='white', linewidth=0.5)\n",
    "axes[1].set_xlabel('Height (px)')\n",
    "axes[1].set_title('Image Height Distribution')\n",
    "\n",
    "# Aspect ratio\n",
    "axes[2].hist(props['aspect'], bins=40, color='mediumseagreen', edgecolor='white', linewidth=0.5)\n",
    "axes[2].axvline(1.0, color='red', linestyle='--', linewidth=1, label='Square')\n",
    "axes[2].set_xlabel('Aspect Ratio (W/H)')\n",
    "axes[2].set_title('Aspect Ratio Distribution')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Training Image Properties', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'eda_04_image_properties.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Aspect ratio range: {props['aspect'].min():.2f} – {props['aspect'].max():.2f}\")\n",
    "print(f\"Portrait (H>W): {(props['height'] > props['width']).sum()} images\")\n",
    "print(f\"Landscape (W>H): {(props['width'] > props['height']).sum()} images\")\n",
    "print(f\"Square: {(props['width'] == props['height']).sum()} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Near-Duplicate Detection\n",
    "\n",
    "Burst photography often produces very similar consecutive images of the same jaguar. Near-duplicates in training:\n",
    "- Help the model learn robust features (slight pose variation)\n",
    "- **Risk overfitting** if duplicates appear in both train and validation splits\n",
    "\n",
    "We use **perceptual hashing** (pHash): a compact fingerprint of image content that's robust to small changes. Images with Hamming distance ≤ 8 are considered near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing perceptual hashes (this may take ~1 minute)...')\n",
    "\n",
    "hashes = {}\n",
    "for fname in train_df['filename']:\n",
    "    img = Image.open(TRAIN_DIR / fname).convert('RGB')\n",
    "    hashes[fname] = imagehash.phash(img)\n",
    "\n",
    "print(f'Computed {len(hashes)} hashes.')\n",
    "\n",
    "# Find near-duplicates (Hamming distance <= 8)\n",
    "filenames = list(hashes.keys())\n",
    "near_dups = []\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    for j in range(i+1, len(filenames)):\n",
    "        dist = hashes[filenames[i]] - hashes[filenames[j]]\n",
    "        if dist <= 8:\n",
    "            near_dups.append({\n",
    "                'img1': filenames[i], 'img2': filenames[j], 'hamming': dist,\n",
    "                'id1': train_df.set_index('filename').loc[filenames[i], 'ground_truth'],\n",
    "                'id2': train_df.set_index('filename').loc[filenames[j], 'ground_truth'],\n",
    "            })\n",
    "\n",
    "dups_df = pd.DataFrame(near_dups)\n",
    "print(f'\\nNear-duplicate pairs found: {len(dups_df)}')\n",
    "if len(dups_df) > 0:\n",
    "    same_id = (dups_df['id1'] == dups_df['id2']).sum()\n",
    "    diff_id = (dups_df['id1'] != dups_df['id2']).sum()\n",
    "    print(f'  Same identity:      {same_id} pairs  ← expected (burst shots)')\n",
    "    print(f'  Different identity: {diff_id} pairs  ← suspicious!')\n",
    "    print(f'\\nHamming distance distribution:')\n",
    "    print(dups_df['hamming'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dups_df) > 0:\n",
    "    # Per-identity near-duplicate count\n",
    "    dup_counts = pd.concat([\n",
    "        dups_df[dups_df['id1'] == dups_df['id2']]['id1'].value_counts()\n",
    "    ]).sort_values(ascending=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Near-dup count per identity\n",
    "    if len(dup_counts) > 0:\n",
    "        dup_counts.plot(kind='bar', ax=axes[0], color='slateblue', edgecolor='white')\n",
    "        axes[0].set_title('Near-Duplicate Pairs per Identity', fontweight='bold')\n",
    "        axes[0].set_xlabel('Jaguar Identity')\n",
    "        axes[0].set_ylabel('Duplicate Pairs')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hamming distance histogram\n",
    "    axes[1].hist(dups_df['hamming'], bins=range(0, 10), color='tomato', \n",
    "                 edgecolor='white', align='left')\n",
    "    axes[1].set_title('Hamming Distance Distribution', fontweight='bold')\n",
    "    axes[1].set_xlabel('Hamming Distance (0=identical, 8=threshold)')\n",
    "    axes[1].set_ylabel('Pair Count')\n",
    "    axes[1].set_xticks(range(0, 9))\n",
    "    \n",
    "    plt.suptitle('Near-Duplicate Analysis', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES / 'eda_05_near_duplicates.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Show example near-duplicate pair\n",
    "    if len(dups_df) > 0:\n",
    "        example = dups_df.iloc[0]\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "        axes[0].imshow(load_img(TRAIN_DIR / example['img1'], size=(256, 256)))\n",
    "        axes[0].set_title(f\"{example['img1']}\\n({example['id1']})\", fontsize=8)\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(load_img(TRAIN_DIR / example['img2'], size=(256, 256)))\n",
    "        axes[1].set_title(f\"{example['img2']}\\n({example['id2']}) | d={example['hamming']}\", fontsize=8)\n",
    "        axes[1].axis('off')\n",
    "        plt.suptitle('Example Near-Duplicate Pair', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES / 'eda_05b_near_duplicate_example.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\nelse:\n",
    "    print('No near-duplicates found at threshold=8.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Pair Analysis\n",
    "\n",
    "The test set contains all 371×370 = 137,270 pairwise comparisons. Since all 31 identities appear in both train and test, we can estimate the **positive/negative ratio** to understand class balance in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image count per identity (inferred from test filenames)\n",
    "# We know the test files (test_0001 to test_0371), but not their identities.\n",
    "# However, we can analyze the pair structure.\n",
    "\n",
    "total_pairs = len(test_df)\n",
    "n_test_imgs = len(test_images)\n",
    "\n",
    "print(f'Total test pairs: {total_pairs:,}')\n",
    "print(f'Unique test images: {n_test_imgs}')\n",
    "print(f'Expected pairs (N*(N-1)): {n_test_imgs*(n_test_imgs-1):,}')\n",
    "print(f'Symmetric pairs: {total_pairs == n_test_imgs*(n_test_imgs-1)}')\n",
    "\n",
    "# Query frequency (each test image queries all others)\n",
    "query_counts = test_df['query_image'].value_counts()\n",
    "print(f'\\nPairs per query image (should all be {n_test_imgs-1}):')\n",
    "print(f'  Min: {query_counts.min()}, Max: {query_counts.max()}, Unique values: {query_counts.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate positive/negative ratio\n",
    "# We know training has 31 identities. If similar distribution holds in test:\n",
    "# Average images per identity in test ~ 371 / 31 = 12\n",
    "# Positive pairs per query ~ 11 (same identity - 1)\n",
    "# Negative pairs per query ~ 370 - 11 = 359\n",
    "\n",
    "avg_test_per_id = n_test_imgs / 31\n",
    "est_positives_per_query = avg_test_per_id - 1\n",
    "est_negatives_per_query = n_test_imgs - avg_test_per_id\n",
    "est_pos_ratio = est_positives_per_query / (n_test_imgs - 1)\n",
    "\n",
    "print('=== ESTIMATED TEST PAIR STATISTICS ===')\n",
    "print(f'Avg images per identity in test : {avg_test_per_id:.1f}')\n",
    "print(f'Est. positive pairs per query   : {est_positives_per_query:.1f}')\n",
    "print(f'Est. negative pairs per query   : {est_negatives_per_query:.1f}')\n",
    "print(f'Est. positive pair ratio        : {est_pos_ratio:.1%}')\n",
    "print()\n",
    "print('=> Roughly 3% of pairs are positives (same jaguar).')\n",
    "print('=> Strong class imbalance — ranking quality matters more than threshold.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Color Characteristics per Identity\n",
    "\n",
    "Jaguar spot patterns have characteristic color profiles. Let's check per-identity mean color values — this will tell us:\n",
    "- Whether color is a useful discriminative feature\n",
    "- Whether camera/lighting introduces identity-confounded biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing per-identity color statistics...')\n",
    "\n",
    "color_records = []\n",
    "for fname, identity in zip(train_df['filename'], train_df['ground_truth']):\n",
    "    img = np.array(Image.open(TRAIN_DIR / fname).convert('RGB').resize((64, 64)))\n",
    "    # Only consider non-white pixels (background was removed)\n",
    "    mask = ~((img[:,:,0] > 240) & (img[:,:,1] > 240) & (img[:,:,2] > 240))\n",
    "    if mask.sum() > 100:\n",
    "        r = img[:,:,0][mask].mean()\n",
    "        g = img[:,:,1][mask].mean()\n",
    "        b = img[:,:,2][mask].mean()\n",
    "        brightness = (img[:,:,0][mask].astype(float)**2 + \n",
    "                      img[:,:,1][mask].astype(float)**2 + \n",
    "                      img[:,:,2][mask].astype(float)**2).mean()**0.5\n",
    "        color_records.append({'filename': fname, 'identity': identity,\n",
    "                               'R': r, 'G': g, 'B': b, 'brightness': brightness,\n",
    "                               'fg_pixels': mask.sum()})\n",
    "\n",
    "color_df = pd.DataFrame(color_records)\n",
    "id_colors = color_df.groupby('identity')[['R','G','B','brightness']].mean()\n",
    "print('Done. Mean color per identity computed.')\n",
    "print(id_colors.describe().round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Sorted by brightness\n",
    "id_sorted = id_colors.sort_values('brightness')\n",
    "\n",
    "x = np.arange(len(id_sorted))\n",
    "w = 0.25\n",
    "axes[0].bar(x - w, id_sorted['R'], w, color='tomato',      label='R', alpha=0.85)\n",
    "axes[0].bar(x,     id_sorted['G'], w, color='mediumseagreen', label='G', alpha=0.85)\n",
    "axes[0].bar(x + w, id_sorted['B'], w, color='steelblue',   label='B', alpha=0.85)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(id_sorted.index, rotation=45, ha='right', fontsize=8)\n",
    "axes[0].set_ylabel('Mean pixel value (foreground only)')\n",
    "axes[0].set_title('Mean RGB Values per Identity (sorted by brightness)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Brightness scatter\n",
    "brightness_by_id = color_df.groupby('identity')['brightness']\n",
    "means = brightness_by_id.mean().sort_values()\n",
    "stds  = brightness_by_id.std().reindex(means.index)\n",
    "\n",
    "axes[1].errorbar(range(len(means)), means.values, yerr=stds.values,\n",
    "                 fmt='o', color='darkorange', ecolor='gray', elinewidth=1, capsize=3,\n",
    "                 markersize=6)\n",
    "axes[1].set_xticks(range(len(means)))\n",
    "axes[1].set_xticklabels(means.index, rotation=45, ha='right', fontsize=8)\n",
    "axes[1].set_ylabel('Brightness (RMS pixel value, foreground)')\n",
    "axes[1].set_title('Brightness per Identity (mean ± std)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Per-Identity Color Characteristics', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'eda_06_color_characteristics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Brightness range across identities: {means.min():.1f} – {means.max():.1f}')\n",
    "print(f'=> Brightness varies by {means.max()-means.min():.1f} units across identities — partially discriminative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Foreground Coverage Analysis\n",
    "\n",
    "Since backgrounds are removed, images are mostly jaguar + white background. Let's check how much of each image is foreground (jaguar pixels) vs background — this tells us how well the segmentation worked and whether padding/cropping strategies matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute foreground fraction per image using 64x64 resized versions\n",
    "fg_records = []\n",
    "for fname in train_df['filename']:\n",
    "    img = np.array(Image.open(TRAIN_DIR / fname).convert('RGBA').resize((64, 64)))\n",
    "    # Use alpha channel if available, else use white-pixel mask\n",
    "    if img.shape[-1] == 4:\n",
    "        fg_frac = (img[:,:,3] > 10).mean()\n",
    "    else:\n",
    "        rgb = img[:,:,:3]\n",
    "        white_mask = (rgb[:,:,0] > 240) & (rgb[:,:,1] > 240) & (rgb[:,:,2] > 240)\n",
    "        fg_frac = (~white_mask).mean()\n",
    "    fg_records.append({'filename': fname, 'fg_frac': fg_frac})\n",
    "\n",
    "fg_df = pd.DataFrame(fg_records).merge(train_df, on='filename')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].hist(fg_df['fg_frac'] * 100, bins=30, color='teal', edgecolor='white')\n",
    "axes[0].set_xlabel('Foreground Coverage (%)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Jaguar Foreground Coverage Distribution', fontweight='bold')\n",
    "\n",
    "# Per-identity average foreground coverage\n",
    "fg_by_id = fg_df.groupby('ground_truth')['fg_frac'].mean().sort_values()\n",
    "fg_by_id.plot(kind='bar', ax=axes[1], color='teal', edgecolor='white')\n",
    "axes[1].set_title('Mean Foreground Coverage per Identity', fontweight='bold')\n",
    "axes[1].set_xlabel('Jaguar Identity')\n",
    "axes[1].set_ylabel('Mean FG Fraction')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "\n",
    "plt.suptitle('Foreground Coverage Analysis', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'eda_07_foreground_coverage.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean foreground coverage: {fg_df[\"fg_frac\"].mean():.1%}')\n",
    "print(f'Images with <30% foreground: {(fg_df[\"fg_frac\"] < 0.3).sum()}')\n",
    "print(f'Images with >80% foreground: {(fg_df[\"fg_frac\"] > 0.8).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metric Walkthrough\n",
    "\n",
    "The competition uses **identity-balanced mAP**. Let's walk through exactly how it's computed with a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(y_true_sorted):\n",
    "    \"\"\"Compute AP from a binary array sorted by descending score.\n",
    "    y_true_sorted[i] = 1 if the i-th ranked item is a positive match.\n",
    "    \"\"\"\n",
    "    n_pos = y_true_sorted.sum()\n",
    "    if n_pos == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    n_correct = 0\n",
    "    for rank, label in enumerate(y_true_sorted, 1):\n",
    "        if label == 1:\n",
    "            n_correct += 1\n",
    "            precisions.append(n_correct / rank)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "# Toy example: query=Marcela, gallery of 10 images (3 Marcela, 7 others)\n",
    "# Perfect model: Marcela images ranked 1, 2, 3\n",
    "# Random model:  Marcela images scattered throughout\n",
    "# Bad model:     Marcela images ranked 8, 9, 10\n",
    "\n",
    "perfect = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "random  = np.array([0, 1, 0, 0, 1, 0, 0, 0, 1, 0])\n",
    "bad     = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
    "\n",
    "print(f'AP (perfect model): {average_precision(perfect):.3f}')  # expect ~1.0\n",
    "print(f'AP (random model):  {average_precision(random):.3f}')   # expect ~0.3-0.5\n",
    "print(f'AP (bad model):     {average_precision(bad):.3f}')      # expect ~0.1\n",
    "\n",
    "print()\n",
    "print('Identity-balanced mAP = mean of AP scores across all 31 jaguar identities')\n",
    "print('Each identity contributes equally regardless of image count.')\n",
    "print()\n",
    "print('=> Models that fail on rare jaguars (Bernard: 13 imgs) will be heavily penalized.')\n",
    "print('=> We must track per-identity AP throughout training, not just overall mAP.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key EDA Takeaways\n",
    "\n",
    "Summary of findings that will guide our modelling approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('KEY EDA FINDINGS')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('1. CLASS IMBALANCE')\n",
    "print(f'   Ratio max/min: {identity_counts.max()}/{identity_counts.min()} = '\n",
    "      f'{identity_counts.max()/identity_counts.min():.1f}x')\n",
    "print('   Identity-balanced mAP equalises this — use identity-stratified CV splits.')\n",
    "print()\n",
    "print('2. NEAR-DUPLICATES')\n",
    "print(f'   Found {len(dups_df)} near-duplicate pairs in training.')\n",
    "print('   Group-by-identity splits essential to avoid data leakage in CV.')\n",
    "print()\n",
    "print('3. TEST PAIR RATIO')\n",
    "print(f'   ~3% of test pairs are positives (same jaguar).')\n",
    "print('   Ranking is what matters — not raw similarity calibration.')\n",
    "print()\n",
    "print('4. IMAGE PROPERTIES')\n",
    "print(f'   Variable resolutions — we need to normalise input size.')\n",
    "print('   Variable aspect ratios — padding to square is safer than stretching.')\n",
    "print('   Background removal is done — white padding is the background.')\n",
    "print()\n",
    "print('5. COLOR DISCRIMINATIVITY')\n",
    "print('   Some per-identity color variation — color features add small signal.')\n",
    "print('   But spot pattern geometry is the primary discriminator.')\n",
    "print()\n",
    "print('6. MODELLING IMPLICATIONS')\n",
    "print('   - Use identity-stratified GroupKFold for CV')\n",
    "print('   - Pad images to square before resizing (not stretch)')\n",
    "print('   - Track per-identity AP, especially for rare jaguars')\n",
    "print('   - Consider flip augmentation carefully (left/right flanks differ)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
